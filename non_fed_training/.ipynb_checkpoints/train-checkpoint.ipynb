{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2cae04a-c059-4acc-ab09-51b7c4a5716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with: helsinki\n",
      "Loading helsinki dataset and selecting F3-C3 (index: 2)...\n",
      "Training from scratch...\n",
      "best model saved...\n",
      "Epoch 1/10\n",
      "Train Loss: 0.5556192168822656 | Train Accuracy: 83.16203143893591\n",
      "Val Loss: 0.34800218160335833 | Val Accuracy: 90.20556227327691\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2561234768050221 | Train Accuracy: 89.20798065296252\n",
      "Val Loss: 0.2529312876554636 | Val Accuracy: 90.44740024183797\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 3/10\n",
      "Train Loss: 0.23854990367992565 | Train Accuracy: 89.29866989117292\n",
      "Val Loss: 0.23232682622396028 | Val Accuracy: 91.17291414752115\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 4/10\n",
      "Train Loss: 0.22188698493230802 | Train Accuracy: 90.43228536880291\n",
      "Val Loss: 0.22213270114018366 | Val Accuracy: 90.44740024183797\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 5/10\n",
      "Train Loss: 0.2126688057413468 | Train Accuracy: 90.85550181378477\n",
      "Val Loss: 0.2128255287042031 | Val Accuracy: 92.38210399032648\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 6/10\n",
      "Train Loss: 0.19913968057013476 | Train Accuracy: 91.26360338573156\n",
      "Val Loss: 0.2060340906565006 | Val Accuracy: 92.86577992744861\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 7/10\n",
      "Train Loss: 0.20016846497758076 | Train Accuracy: 91.38452237001209\n",
      "Val Loss: 0.18859097648125428 | Val Accuracy: 92.38210399032648\n",
      "-----------------------------\n",
      "Epoch 8/10\n",
      "Train Loss: 0.1686309685644049 | Train Accuracy: 93.2738814993954\n",
      "Val Loss: 0.19013109573951134 | Val Accuracy: 94.19588875453447\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 9/10\n",
      "Train Loss: 0.1694430188777355 | Train Accuracy: 92.88089480048367\n",
      "Val Loss: 0.17839971643227798 | Val Accuracy: 93.10761789600967\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 10/10\n",
      "Train Loss: 0.16549434215546802 | Train Accuracy: 93.06227327690448\n",
      "Val Loss: 0.17211704987746018 | Val Accuracy: 91.65659008464328\n",
      "-----------------------------\n",
      "Test Loss: 0.1490 | Test Acc: 93.60%\n",
      "SEED = 42 DONE\n",
      "===========\n",
      "Dealing with: helsinki\n",
      "Loading helsinki dataset and selecting F3-C3 (index: 2)...\n",
      "Training from scratch...\n",
      "best model saved...\n",
      "Epoch 1/10\n",
      "Train Loss: 0.5506774241534563 | Train Accuracy: 75.92200725513905\n",
      "Val Loss: 0.32788386253210217 | Val Accuracy: 88.75453446191052\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2697617752620807 | Train Accuracy: 88.37666263603386\n",
      "Val Loss: 0.24779993295669556 | Val Accuracy: 88.99637243047158\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 3/10\n",
      "Train Loss: 0.2337488687525575 | Train Accuracy: 89.99395405078597\n",
      "Val Loss: 0.2430020158107464 | Val Accuracy: 88.39177750906893\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 4/10\n",
      "Train Loss: 0.216059764703879 | Train Accuracy: 90.85550181378477\n",
      "Val Loss: 0.2317598840365043 | Val Accuracy: 89.48004836759371\n",
      "-----------------------------\n",
      "Epoch 5/10\n",
      "Train Loss: 0.20415803224134904 | Train Accuracy: 91.0671100362757\n",
      "Val Loss: 0.23610914154694632 | Val Accuracy: 91.53567110036276\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 6/10\n",
      "Train Loss: 0.19553330533493024 | Train Accuracy: 91.3391777509069\n",
      "Val Loss: 0.21883879601955414 | Val Accuracy: 89.60096735187425\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 7/10\n",
      "Train Loss: 0.1836569753403847 | Train Accuracy: 92.17049576783555\n",
      "Val Loss: 0.2074004474740762 | Val Accuracy: 90.9310761789601\n",
      "-----------------------------\n",
      "Epoch 8/10\n",
      "Train Loss: 0.17449470437490022 | Train Accuracy: 92.59371221281741\n",
      "Val Loss: 0.2228467556146475 | Val Accuracy: 87.78718258766627\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 9/10\n",
      "Train Loss: 0.17162299156188965 | Train Accuracy: 92.71463119709794\n",
      "Val Loss: 0.19141066934053713 | Val Accuracy: 92.38210399032648\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 10/10\n",
      "Train Loss: 0.16135863361593622 | Train Accuracy: 93.25876662636034\n",
      "Val Loss: 0.1882611707999156 | Val Accuracy: 90.08464328899638\n",
      "-----------------------------\n",
      "Test Loss: 0.1694 | Test Acc: 90.70%\n",
      "SEED = 43 DONE\n",
      "===========\n",
      "Dealing with: helsinki\n",
      "Loading helsinki dataset and selecting F3-C3 (index: 2)...\n",
      "Training from scratch...\n",
      "best model saved...\n",
      "Epoch 1/10\n",
      "Train Loss: 0.5525233044933814 | Train Accuracy: 84.71886336154776\n",
      "Val Loss: 0.32011564190571123 | Val Accuracy: 88.63361547762999\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2661766243668703 | Train Accuracy: 88.89056831922612\n",
      "Val Loss: 0.24905003263400152 | Val Accuracy: 89.72188633615478\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 3/10\n",
      "Train Loss: 0.22670313601310438 | Train Accuracy: 90.02418379685611\n",
      "Val Loss: 0.23200779121655685 | Val Accuracy: 92.01934703748489\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 4/10\n",
      "Train Loss: 0.22094470964601406 | Train Accuracy: 90.62877871825877\n",
      "Val Loss: 0.21798069775104523 | Val Accuracy: 91.41475211608223\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 5/10\n",
      "Train Loss: 0.21292353271005246 | Train Accuracy: 90.62877871825877\n",
      "Val Loss: 0.20933063672139093 | Val Accuracy: 90.81015719467956\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 6/10\n",
      "Train Loss: 0.20041126947706708 | Train Accuracy: 91.42986698911729\n",
      "Val Loss: 0.19988431265720955 | Val Accuracy: 90.5683192261185\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 7/10\n",
      "Train Loss: 0.20251734404323193 | Train Accuracy: 91.09733978234583\n",
      "Val Loss: 0.18146083503961563 | Val Accuracy: 92.62394195888754\n",
      "-----------------------------\n",
      "Epoch 8/10\n",
      "Train Loss: 0.16838648127248654 | Train Accuracy: 92.89600967351875\n",
      "Val Loss: 0.18192520852272326 | Val Accuracy: 93.47037484885126\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 9/10\n",
      "Train Loss: 0.16477840254083276 | Train Accuracy: 93.3645707376058\n",
      "Val Loss: 0.16112422943115234 | Val Accuracy: 92.14026602176541\n",
      "-----------------------------\n",
      "Epoch 10/10\n",
      "Train Loss: 0.16660627049322313 | Train Accuracy: 92.68440145102781\n",
      "Val Loss: 0.17056284615626702 | Val Accuracy: 93.9540507859734\n",
      "-----------------------------\n",
      "Test Loss: 0.1684 | Test Acc: 93.72%\n",
      "SEED = 44 DONE\n",
      "===========\n",
      "Dealing with: helsinki\n",
      "Loading helsinki dataset and selecting F3-C3 (index: 2)...\n",
      "Training from scratch...\n",
      "best model saved...\n",
      "Epoch 1/10\n",
      "Train Loss: 0.5487765219922249 | Train Accuracy: 83.34340991535672\n",
      "Val Loss: 0.2912394255399704 | Val Accuracy: 89.96372430471584\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2564869253681256 | Train Accuracy: 88.61850060459493\n",
      "Val Loss: 0.2314076950916877 | Val Accuracy: 88.39177750906893\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 3/10\n",
      "Train Loss: 0.22432166173194462 | Train Accuracy: 90.2962515114873\n",
      "Val Loss: 0.22743628002130067 | Val Accuracy: 92.38210399032648\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 4/10\n",
      "Train Loss: 0.2060550206269209 | Train Accuracy: 91.36940749697702\n",
      "Val Loss: 0.21080020242012465 | Val Accuracy: 89.72188633615478\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1926862962830525 | Train Accuracy: 91.55078597339782\n",
      "Val Loss: 0.20395679428027227 | Val Accuracy: 91.77750906892382\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 6/10\n",
      "Train Loss: 0.1961184607532162 | Train Accuracy: 91.09733978234583\n",
      "Val Loss: 0.18979319815452284 | Val Accuracy: 91.29383313180169\n",
      "-----------------------------\n",
      "Epoch 7/10\n",
      "Train Loss: 0.18694720273980728 | Train Accuracy: 91.77750906892382\n",
      "Val Loss: 0.2274042832163664 | Val Accuracy: 93.5912938331318\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 8/10\n",
      "Train Loss: 0.17572162851977807 | Train Accuracy: 92.29141475211608\n",
      "Val Loss: 0.16716302300875002 | Val Accuracy: 92.86577992744861\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 9/10\n",
      "Train Loss: 0.16322214847717148 | Train Accuracy: 93.15296251511488\n",
      "Val Loss: 0.1608318669291643 | Val Accuracy: 93.10761789600967\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 10/10\n",
      "Train Loss: 0.1564155972849291 | Train Accuracy: 93.42503022974607\n",
      "Val Loss: 0.1583486136335593 | Val Accuracy: 94.07496977025393\n",
      "-----------------------------\n",
      "Test Loss: 0.1661 | Test Acc: 93.48%\n",
      "SEED = 45 DONE\n",
      "===========\n",
      "Dealing with: helsinki\n",
      "Loading helsinki dataset and selecting F3-C3 (index: 2)...\n",
      "Training from scratch...\n",
      "best model saved...\n",
      "Epoch 1/10\n",
      "Train Loss: 0.5598917090548918 | Train Accuracy: 84.1142684401451\n",
      "Val Loss: 0.3122544609583341 | Val Accuracy: 89.48004836759371\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 2/10\n",
      "Train Loss: 0.2697428312295905 | Train Accuracy: 87.80229746070133\n",
      "Val Loss: 0.23835915670945093 | Val Accuracy: 89.23821039903265\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 3/10\n",
      "Train Loss: 0.22462641247189963 | Train Accuracy: 90.11487303506651\n",
      "Val Loss: 0.22706218636952913 | Val Accuracy: 89.23821039903265\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 4/10\n",
      "Train Loss: 0.21803432158552682 | Train Accuracy: 90.5229746070133\n",
      "Val Loss: 0.22426462517334864 | Val Accuracy: 91.17291414752115\n",
      "-----------------------------\n",
      "Epoch 5/10\n",
      "Train Loss: 0.21567466195959312 | Train Accuracy: 90.67412333736397\n",
      "Val Loss: 0.2364864945411682 | Val Accuracy: 92.26118500604595\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 6/10\n",
      "Train Loss: 0.2184104211628437 | Train Accuracy: 90.76481257557437\n",
      "Val Loss: 0.2108257802633139 | Val Accuracy: 92.14026602176541\n",
      "-----------------------------\n",
      "Epoch 7/10\n",
      "Train Loss: 0.19207276891057307 | Train Accuracy: 91.7926239419589\n",
      "Val Loss: 0.21795341945611513 | Val Accuracy: 92.503022974607\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 8/10\n",
      "Train Loss: 0.19321827275248674 | Train Accuracy: 91.62636033857315\n",
      "Val Loss: 0.18382801574010116 | Val Accuracy: 92.26118500604595\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 9/10\n",
      "Train Loss: 0.185308267075855 | Train Accuracy: 92.11003627569528\n",
      "Val Loss: 0.18142081109377053 | Val Accuracy: 93.2285368802902\n",
      "-----------------------------\n",
      "best model saved...\n",
      "Epoch 10/10\n",
      "Train Loss: 0.1798232837150303 | Train Accuracy: 92.71463119709794\n",
      "Val Loss: 0.16633682698011398 | Val Accuracy: 92.98669891172914\n",
      "-----------------------------\n",
      "Test Loss: 0.1612 | Test Acc: 92.75%\n",
      "SEED = 46 DONE\n",
      "===========\n",
      "[93.59903382 90.70048309 93.71980676 93.47826087 92.75362319]\n",
      "Mean Accuracy from 5 runs: 92.85024154589372\n",
      "SD of Accuracy from 5 runs: 1.1262360605676125\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "from utils.datasets_statistics import get_channel_index_for_dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from utils.utils import *\n",
    "from utils.dataloader import get_dataloader, get_balanced_dataloader\n",
    "from utils.model_training import *\n",
    "from utils.tiny_sleep_net import TinySleepNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def main(dataset_path, model_save_dir, model_save_name, dataset_index, nch_dataset_path):\n",
    "    all_combined_accuracies = [] \n",
    "    n_epochs = 10\n",
    "    patience = 25\n",
    "    \n",
    "    for seed in [42, 43, 44, 45, 46]:\n",
    "        set_seed(seed)\n",
    "        best_val_loss = float(\"inf\")\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        channel_name = 'F3-C3'\n",
    "        batch_size = 64 #for training later\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model_save_dir = model_save_dir\n",
    "        os.makedirs(model_save_dir, exist_ok = True)\n",
    "        model_save_name = f'{model_save_name}_{seed}.pth'\n",
    "\n",
    "        datasets_to_merge = [('chb', True), ('helsinki', True), ('nch', False), ('sienna', True)] \n",
    "\n",
    "        all_X_train, all_y_train = [], []\n",
    "        all_X_val, all_y_val = [], []\n",
    "        all_X_test, all_y_test = [], []\n",
    "\n",
    "        train_names, val_names, test_names = [], [], []  \n",
    "\n",
    "        all_data = [] \n",
    "\n",
    "        datasets_to_merge = datasets_to_merge[dataset_index:dataset_index + 1]\n",
    "\n",
    "        for data in datasets_to_merge:\n",
    "            dataset_name = data[0]\n",
    "            scale_to_nch = data[1]\n",
    "            print ('Dealing with:', dataset_name)\n",
    "            file = f'{dataset_path}'\n",
    "\n",
    "            channel_index = get_channel_index_for_dataset(dataset_name, channel_name)\n",
    "\n",
    "            print(f\"Loading {dataset_name} dataset and selecting {channel_name} (index: {channel_index})...\")\n",
    "\n",
    "            X_sub, y_sub = load_patientwise_file(file, channel_index, scale_to_nch, nch_dataset_path)\n",
    "\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test) = stratified_train_val_test_split(X_sub, y_sub, random_state = seed)\n",
    "\n",
    "            # Append dataset splits\n",
    "            all_X_train.append(X_train)\n",
    "            all_y_train.append(y_train)\n",
    "\n",
    "            all_X_val.append(X_val)\n",
    "            all_y_val.append(y_val)\n",
    "\n",
    "            all_X_test.append(X_test)\n",
    "            all_y_test.append(y_test)\n",
    "\n",
    "            all_data.append(X_train)\n",
    "            all_data.append(X_val)\n",
    "            all_data.append(X_test)\n",
    "\n",
    "            # Append corresponding dataset names\n",
    "            train_names.extend([dataset_name] * len(X_train))  \n",
    "            val_names.extend([dataset_name] * len(X_val))\n",
    "            test_names.extend([dataset_name] * len(X_test))\n",
    "\n",
    "        X_train = np.concatenate(all_X_train, axis=0)\n",
    "        y_train = np.concatenate(all_y_train, axis=0)\n",
    "\n",
    "        X_val = np.concatenate(all_X_val, axis=0)\n",
    "        y_val = np.concatenate(all_y_val, axis=0)\n",
    "\n",
    "        X_test = np.concatenate(all_X_test, axis=0)\n",
    "        y_test = np.concatenate(all_y_test, axis=0)\n",
    "\n",
    "        all_data = np.concatenate(all_data, axis = 0)\n",
    "\n",
    "        global_mean, global_sd = np.mean(all_data, axis = None), np.std(all_data, axis = None)\n",
    "\n",
    "        X_train, X_val, X_test = standardize_data(X_train, global_mean, global_sd), standardize_data(X_val, global_mean, global_sd), standardize_data(X_test, global_mean, global_sd)\n",
    "\n",
    "        train_names = np.array(train_names)\n",
    "        val_names = np.array(val_names)\n",
    "        test_names = np.array(test_names)\n",
    "\n",
    "        unique_labels = np.unique(y_train)\n",
    "        class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=y_train)\n",
    "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "        train_dataloader = get_balanced_dataloader(X_train, y_train, train_names, batch_size, shuffle = True)\n",
    "        val_dataloader = get_dataloader(X_val, y_val, batch_size, shuffle = False)\n",
    "        test_dataloader = get_dataloader(X_test, y_test, batch_size, shuffle = False)\n",
    "\n",
    "        sleep_model = TinySleepNet(num_classes = 2, Fs = 12, kernel_size = 4).to(device)    \n",
    "\n",
    "        print (\"Training from scratch...\")\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(weight = class_weights_tensor) \n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, sleep_model.parameters()), lr=4e-5, weight_decay=1e-6)    \n",
    "\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss, train_acc = train(sleep_model, device, train_dataloader, loss, optimizer)\n",
    "            val_loss, val_acc = validate(sleep_model, device, val_dataloader, loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0 \n",
    "                print (\"best model saved...\")\n",
    "                torch.save(sleep_model.state_dict(), f\"{model_save_dir}/{model_save_name}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1 \n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\\nTrain Loss: {train_loss} | Train Accuracy: {train_acc}\")\n",
    "            print(f\"Val Loss: {val_loss} | Val Accuracy: {val_acc}\")\n",
    "            print (\"-----------------------------\")\n",
    "\n",
    "        test_loss, test_acc  = test(sleep_model, device, test_dataloader, loss)\n",
    "        print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "        all_combined_accuracies.append(test_acc)\n",
    "    \n",
    "        print (f\"SEED = {seed} DONE\")\n",
    "        print (\"===========\")\n",
    "        \n",
    "    all_combined_accuracies = np.array(all_combined_accuracies)\n",
    "    print (all_combined_accuracies)\n",
    "    print (\"Mean Accuracy from 5 runs:\", np.mean(all_combined_accuracies))\n",
    "    print (\"SD of Accuracy from 5 runs:\", np.std(all_combined_accuracies))\n",
    "    \n",
    "\n",
    "path = '../DATA/helsinki_patientwise.h5'\n",
    "model_save_dir = 'saved_models_local'\n",
    "dataset_index = 1 \n",
    "nch_dataset_path = '../DATA/nch_patientwise.h5'\n",
    "model_save_name = 'helsinki'\n",
    "main(path, model_save_dir, model_save_name, dataset_index, nch_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268efffb-be22-48ca-9a7a-dcecd06fff23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a68e9cea-b485-49b1-9ee7-c4b57dabd44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import *\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "min_val, max_val = nch_min_max(7, '../DATA/nch_patientwise.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a3c15-7698-45b2-b124-93ab8c2e418d",
   "metadata": {},
   "source": [
    "print (min_val, max_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d5b61-7da0-4ff4-bbbf-a7a3cf803213",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Train All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d032b63-067c-4fcd-9862-7c1d969643fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "from utils.datasets_statistics import get_channel_index_for_dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from utils.utils import *\n",
    "from utils.dataloader import get_dataloader, get_balanced_dataloader\n",
    "from utils.model_training import *\n",
    "from utils.tiny_sleep_net import TinySleepNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "def main(chb, helsinki, nch, sienna, model_save_dir, model_save_name):\n",
    "    all_combined_accuracies = [] \n",
    "    n_epochs = 10\n",
    "    patience = 25\n",
    "    \n",
    "    for seed in [42, 43, 44, 45, 46]:\n",
    "        set_seed(seed)\n",
    "        best_val_loss = float(\"inf\")\n",
    "        epochs_no_improve = 0\n",
    "\n",
    "        channel_name = 'F3-C3'\n",
    "        batch_size = 64 #for training later\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        model_save_dir = model_save_dir\n",
    "        os.makedirs(model_save_dir, exist_ok = True)\n",
    "        model_save_name = f'{model_save_name}_{seed}.pth'\n",
    "\n",
    "        datasets_to_merge = [('chb', True), ('helsinki', True), ('nch', False), ('sienna', True)] \n",
    "        datasets_paths = [chb, helsinki, nch, sienna]\n",
    "\n",
    "        all_X_train, all_y_train = [], []\n",
    "        all_X_val, all_y_val = [], []\n",
    "        all_X_test, all_y_test = [], []\n",
    "\n",
    "        train_names, val_names, test_names = [], [], []  \n",
    "\n",
    "        all_data = [] \n",
    "\n",
    "        for idx, data in enumerate(datasets_to_merge):\n",
    "            dataset_name = data[0]\n",
    "            scale_to_nch = data[1]\n",
    "            print ('Dealing with:', dataset_name)\n",
    "            file = f'{dataset_paths[idx]}'\n",
    "\n",
    "            channel_index = get_channel_index_for_dataset(dataset_name, channel_name)\n",
    "\n",
    "            print(f\"Loading {dataset_name} dataset and selecting {channel_name} (index: {channel_index})...\")\n",
    "\n",
    "            X_sub, y_sub = load_patientwise_file(file, channel_index, scale_to_nch, nch)\n",
    "\n",
    "            (X_train, y_train), (X_val, y_val), (X_test, y_test) = stratified_train_val_test_split(X_sub, y_sub, random_state = seed)\n",
    "\n",
    "            # Append dataset splits\n",
    "            all_X_train.append(X_train)\n",
    "            all_y_train.append(y_train)\n",
    "\n",
    "            all_X_val.append(X_val)\n",
    "            all_y_val.append(y_val)\n",
    "\n",
    "            all_X_test.append(X_test)\n",
    "            all_y_test.append(y_test)\n",
    "\n",
    "            all_data.append(X_train)\n",
    "            all_data.append(X_val)\n",
    "            all_data.append(X_test)\n",
    "\n",
    "            # Append corresponding dataset names\n",
    "            train_names.extend([dataset_name] * len(X_train))  \n",
    "            val_names.extend([dataset_name] * len(X_val))\n",
    "            test_names.extend([dataset_name] * len(X_test))\n",
    "\n",
    "        X_train = np.concatenate(all_X_train, axis=0)\n",
    "        y_train = np.concatenate(all_y_train, axis=0)\n",
    "\n",
    "        X_val = np.concatenate(all_X_val, axis=0)\n",
    "        y_val = np.concatenate(all_y_val, axis=0)\n",
    "\n",
    "        X_test = np.concatenate(all_X_test, axis=0)\n",
    "        y_test = np.concatenate(all_y_test, axis=0)\n",
    "\n",
    "        all_data = np.concatenate(all_data, axis = 0)\n",
    "\n",
    "        global_mean, global_sd = np.mean(all_data, axis = None), np.std(all_data, axis = None)\n",
    "\n",
    "        X_train, X_val, X_test = standardize_data(X_train, global_mean, global_sd), standardize_data(X_val, global_mean, global_sd), standardize_data(X_test, global_mean, global_sd)\n",
    "\n",
    "        train_names = np.array(train_names)\n",
    "        val_names = np.array(val_names)\n",
    "        test_names = np.array(test_names)\n",
    "\n",
    "        unique_labels = np.unique(y_train)\n",
    "        class_weights = compute_class_weight(class_weight='balanced', classes=unique_labels, y=y_train)\n",
    "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "        train_dataloader = get_balanced_dataloader(X_train, y_train, train_names, batch_size, shuffle = True)\n",
    "        val_dataloader = get_dataloader(X_val, y_val, batch_size, shuffle = False)\n",
    "        test_dataloader = get_dataloader(X_test, y_test, batch_size, shuffle = False)\n",
    "\n",
    "        sleep_model = TinySleepNet(num_classes = 2, Fs = 12, kernel_size = 4).to(device)    \n",
    "\n",
    "        print (\"Training from scratch...\")\n",
    "\n",
    "        loss = nn.CrossEntropyLoss(weight = class_weights_tensor) \n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, sleep_model.parameters()), lr=4e-5, weight_decay=1e-6)    \n",
    "\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss, train_acc = train(sleep_model, device, train_dataloader, loss, optimizer)\n",
    "            val_loss, val_acc = validate(sleep_model, device, val_dataloader, loss)\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0 \n",
    "                print (\"best model saved...\")\n",
    "                torch.save(sleep_model.state_dict(), f\"{model_save_dir}/{model_save_name}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1 \n",
    "\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}. No improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}\\nTrain Loss: {train_loss} | Train Accuracy: {train_acc}\")\n",
    "            print(f\"Val Loss: {val_loss} | Val Accuracy: {val_acc}\")\n",
    "            print (\"-----------------------------\")\n",
    "\n",
    "        test_loss, test_acc  = test(sleep_model, device, test_dataloader, loss)\n",
    "        print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "        all_combined_accuracies.append(test_acc)\n",
    "    \n",
    "        print (f\"SEED = {seed} DONE\")\n",
    "        print (\"===========\")\n",
    "        \n",
    "    all_combined_accuracies = np.array(all_combined_accuracies)\n",
    "    print (all_combined_accuracies)\n",
    "    print (\"Mean Accuracy from 5 runs:\", np.mean(all_combined_accuracies))\n",
    "    print (\"SD of Accuracy from 5 runs:\", np.std(all_combined_accuracies))\n",
    "    \n",
    "\n",
    "chb_path = '../DATA/chb_patientwise.h5'\n",
    "helsinki_path = '../DATA/helsinki_patientwise.h5'\n",
    "nch_path = '../DATA/nch_patientwise.h5'\n",
    "sienna_path = '../DATA/siena_patientwise.h5'\n",
    "model_save_dir = 'saved_models_local'\n",
    "model_save_name = 'train_all'\n",
    "main(chb_path, helsinki_path, nch_path, sienna_path, model_save_dir, model_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fc042-df58-4b67-93d4-f6b0b10d4d93",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Test Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53af6074-04f1-466b-a525-2b8571d6a455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import h5py\n",
    "import os\n",
    "import logging\n",
    "from utils.datasets_statistics import get_channel_index_for_dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from utils.utils import *\n",
    "from utils.dataloader import get_dataloader, get_balanced_dataloader\n",
    "from utils.model_training import *\n",
    "from utils.tiny_sleep_net import TinySleepNet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def setup_logger(log_path):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers.clear()  # prevent duplicate logs\n",
    "    \n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    console = logging.StreamHandler()\n",
    "    console.setFormatter(formatter)\n",
    "    logger.addHandler(console)\n",
    "\n",
    "    fh = logging.FileHandler(log_path)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "\n",
    "\n",
    "all_test_accuracies = [] \n",
    "batch_size = 64 \n",
    "\n",
    "def main(chb, helsinki, nch, sienna, test_index, use_index, model_save_dir, model_save_name):\n",
    "\n",
    "    for seed in [42,43,44,45,46]:\n",
    "        print (\"SEED = \", seed)\n",
    "        channel_name = 'F3-C3'\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        set_seed(seed)\n",
    "\n",
    "        datasets_to_merge = [('chb', True), ('helsinki', True), ('nch', False), ('sienna', True)]\n",
    "        dataset_paths = [chb, helsinki, nch, sienna]\n",
    "        \n",
    "        TEST_ON = datasets_to_merge[test_index]\n",
    "        USE = datasets_to_merge[use_index]\n",
    "\n",
    "\n",
    "        for idx, data in enumerate(datasets_to_merge):\n",
    "            dataset_name = data[0]\n",
    "            scale_to_nch = data[1]\n",
    "\n",
    "            if dataset_name not in [TEST_ON]:\n",
    "                print (\"No need for\", dataset_name)\n",
    "                continue\n",
    "\n",
    "            print ('Testing On:', dataset_name)\n",
    "            file = f'{dataset_paths[idx]'\n",
    "\n",
    "            channel_index = get_channel_index_for_dataset(dataset_name, channel_name)\n",
    "\n",
    "            X_sub, y_sub = load_patientwise_file(file, channel_index, scale_to_nch)\n",
    "\n",
    "            if (dataset_name == TEST_ON):\n",
    "                save_test_data = X_sub\n",
    "                save_test_labels = y_sub\n",
    "\n",
    "\n",
    "        test_mean = np.mean(save_test_data, axis = None)\n",
    "        test_sd = np.std(save_test_data, axis = None)\n",
    "\n",
    "        standardized_test_data = standardize_data(save_test_data, test_mean, test_sd)\n",
    "\n",
    "        test_dataloader = get_dataloader(standardized_test_data, save_test_labels, batch_size, shuffle = False)\n",
    "\n",
    "        sleep_model = TinySleepNet(num_classes = 2, Fs = 12, kernel_size = 4).to(device) \n",
    "\n",
    "        model_name = f'{model_save_name}_{seed}'\n",
    "\n",
    "        model_save_name = f'{model_save_dir}/{model_name}.pth'\n",
    "        sleep_model.load_state_dict(torch.load(model_save_name))\n",
    "\n",
    "        loss = nn.CrossEntropyLoss() \n",
    "\n",
    "        print (\"Testing...\")\n",
    "        test_loss, test_acc = test(sleep_model, device, test_dataloader, loss)\n",
    "        print (test_acc)\n",
    "        all_test_accuracies.append(test_acc)\n",
    "        print (\"******************\")\n",
    "\n",
    "\n",
    "    print (\"***FINAL RESULTS:****\")  \n",
    "    print (\"LOAD:\", USE)\n",
    "    print (\"EVALUATE:\", TEST_ON)\n",
    "    all_test_accuracies = np.array(all_test_accuracies)\n",
    "    print (\"Mean:\", np.mean(all_test_accuracies))\n",
    "    print (\"Std:\", np.std(all_test_accuracies))\n",
    "\n",
    "chb_path = '../DATA/chb_patientwise.h5'\n",
    "helsinki_path = '../DATA/helsinki_patientwise.h5'\n",
    "nch_path = '../DATA/nch_patientwise.h5'\n",
    "sienna_path = '../DATA/siena_patientwise.h5'\n",
    "model_save_dir = 'saved_models_local'\n",
    "model_save_name = 'train_all'\n",
    "test_index = 2  # 0 = chb, 1 = helsinki, 2 = nch, 3 = sienna\n",
    "use_index = 1\n",
    "main(chb_path, helsinki_path, nch_path, sienna_path, 2, 1 model_save_dir, model_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5132cf8-b5a0-4ba1-bcec-e1dc6ec121c7",
   "metadata": {},
   "source": [
    "## Test Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eabe4f-7f55-42e1-8f4a-b3899f4db2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from utils.datasets_statistics import get_channel_index_for_dataset\n",
    "from utils.utils import *\n",
    "from utils.dataloader import get_dataloader\n",
    "from utils.model_training import test\n",
    "from utils.tiny_sleep_net import TinySleepNet\n",
    "\n",
    "\n",
    "def setup_logger(log_path):\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    formatter = logging.Formatter('%(message)s')\n",
    "    console = logging.StreamHandler()\n",
    "    console.setFormatter(formatter)\n",
    "    logger.addHandler(console)\n",
    "\n",
    "    fh = logging.FileHandler(log_path)\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "\n",
    "def main(chb, helsinki, nch, sienna, test_index, model_save_dir, model_save_name, log_path):\n",
    "    setup_logger(log_path)\n",
    "\n",
    "    batch_size = 64\n",
    "    channel_name = 'F3-C3'\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    datasets_to_merge = [('chb', True), ('helsinki', True), ('nch', False), ('sienna', True)]\n",
    "    dataset_paths = [chb, helsinki, nch, sienna]\n",
    "    test_dataset_name, test_scale = datasets_to_merge[test_index]\n",
    "\n",
    "    all_combined_accuracies, all_combined_f1s, all_combined_aurocs = [], [], []\n",
    "\n",
    "    for seed in [42, 43, 44, 45, 46]:\n",
    "        logging.info(f\"\\n=== SEED: {seed} ===\")\n",
    "        set_seed(seed)\n",
    "\n",
    "        all_X_train, all_X_test, all_y_test, all_examples = [], [], [], []\n",
    "        test_names = []\n",
    "\n",
    "        for idx, (dataset_name, scale_to_nch) in enumerate(datasets_to_merge):\n",
    "            file = dataset_paths[idx]\n",
    "            channel_index = get_channel_index_for_dataset(dataset_name, channel_name)\n",
    "\n",
    "            X_sub, y_sub = load_patientwise_file(file, channel_index, scale_to_nch)\n",
    "            (X_train, _), (X_val, _), (X_test, y_test) = stratified_train_val_test_split(X_sub, y_sub, random_state=seed)\n",
    "\n",
    "            if dataset_name == test_dataset_name:\n",
    "                save_test_data = X_test\n",
    "                save_test_labels = y_test\n",
    "\n",
    "            all_X_train.append(X_train)\n",
    "            all_X_test.append(X_test)\n",
    "            all_y_test.append(y_test)\n",
    "            all_examples.extend([X_train, X_val, X_test])\n",
    "            test_names.extend([dataset_name] * len(X_test))\n",
    "\n",
    "        all_examples = np.concatenate(all_examples, axis=0)\n",
    "        global_mean, global_sd = np.mean(all_examples), np.std(all_examples)\n",
    "\n",
    "        logging.info(f\"Standardizing dataset {test_dataset_name}\")\n",
    "        logging.info(f\"Global mean: {global_mean:.4f}, Global std: {global_sd:.4f}\")\n",
    "\n",
    "        standardized_test_data = (save_test_data - global_mean) / global_sd\n",
    "        test_dataloader = get_dataloader(standardized_test_data, save_test_labels, batch_size, shuffle=False)\n",
    "\n",
    "        sleep_model = TinySleepNet(num_classes=2, Fs=12, kernel_size=4).to(device)\n",
    "        model_path = os.path.join(model_save_dir, f\"{model_save_name}_{seed}.pth\")\n",
    "        sleep_model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        logging.info(\"Testing model...\")\n",
    "        test_loss, test_acc, f1, auroc = test(sleep_model, device, test_dataloader, loss)\n",
    "\n",
    "        logging.info(f\"Test Accuracy: {test_acc:.2f} | F1 Score: {f1:.2f} | AUROC: {auroc:.2f}\")\n",
    "        all_combined_accuracies.append(test_acc)\n",
    "        all_combined_f1s.append(f1)\n",
    "        all_combined_aurocs.append(auroc)\n",
    "\n",
    "    logging.info(\"\\n********** FINAL RESULTS ************\")\n",
    "    logging.info(f\"Evaluated on: {test_dataset_name}\")\n",
    "    logging.info(f\"Accuracy Mean: {np.mean(all_combined_accuracies):.2f} | Std: {np.std(all_combined_accuracies):.2f}\")\n",
    "    logging.info(f\"F1 Score Mean: {np.mean(all_combined_f1s):.2f} | Std: {np.std(all_combined_f1s):.2f}\")\n",
    "    logging.info(f\"AUROC Mean: {np.mean(all_combined_aurocs):.2f} | Std: {np.std(all_combined_aurocs):.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Evaluate pretrained TinySleepNet models on different EEG datasets.\",\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "\n",
    "    parser.add_argument('--chb', type=str, default='../DATA/chb_patientwise.h5', help='Path to CHB dataset')\n",
    "    parser.add_argument('--helsinki', type=str, default='../DATA/helsinki_patientwise.h5', help='Path to Helsinki dataset')\n",
    "    parser.add_argument('--nch', type=str, default='../DATA/nch_patientwise.h5', help='Path to NCH dataset')\n",
    "    parser.add_argument('--sienna', type=str, default='../DATA/siena_patientwise.h5', help='Path to Sienna dataset')\n",
    "    parser.add_argument('--test_index', type=int, choices=[0, 1, 2, 3], default=2,\n",
    "                        help='Index of dataset to test on (0 = chb, 1 = helsinki, 2 = nch, 3 = sienna)')\n",
    "    parser.add_argument('--model_save_dir', type=str, default='saved_models_local', help='Directory with saved models')\n",
    "    parser.add_argument('--model_save_name', type=str, default='train_all', help='Base name for saved models (seed will be appended)')\n",
    "    parser.add_argument('--log_path', type=str, default='cross_eval_log.txt', help='Path to log file')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args.chb, args.helsinki, args.nch, args.sienna, args.test_index,\n",
    "         args.model_save_dir, args.model_save_name, args.log_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ssenv)",
   "language": "python",
   "name": "ssenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
